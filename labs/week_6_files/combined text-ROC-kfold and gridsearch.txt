âœ… Trains multiple models automatically.
âœ… Predicts class labels & probabilities.
âœ… Computes ROC curves & AUC scores.
âœ… Prints confusion matrices & classification reports.
âœ… Plots all ROC curves in one graph for comparison.

>>

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# Load data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define classifiers in a dictionary
classifiers = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(max_depth=5),
    "Random Forest": RandomForestClassifier(n_estimators=100),
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
    "Support Vector Machine": SVC(probability=True)
}

# Create a plot for ROC curves
plt.figure(figsize=(10, 7))

for name, model in classifiers.items():
    # Train the model
    model.fit(X_train, y_train)
    
    # Predict probabilities (for ROC curve)
    y_probs = model.predict_proba(X_test)[:, 1]  # Probability of positive class
    
    # Predict class labels (for confusion matrix)
    y_pred = model.predict(X_test)

    # Compute ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_probs)
    roc_auc = auc(fpr, tpr)
    
    # Plot ROC curve
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")
    
    # Print Confusion Matrix
    print(f"ðŸ”¹ {name} - Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print(f"ðŸ”¹ {name} - Classification Report:\n", classification_report(y_test, y_pred))
    print("="*60)

# Plot ROC curve settings
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")  # Diagonal reference line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.show()



-----Hyperparameter Tuning with GridSearchCV
Use GridSearchCV to find the best k value and distance metric automatically.


from sklearn.model_selection import GridSearchCV

# Define hyperparameters to test
param_grid = {
    'n_neighbors': range(1, 30, 2),  # Odd values of k from 1 to 29
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

# Run GridSearchCV
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best parameters and score
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Accuracy: {grid_search.best_score_:.4f}")


>>> his will test multiple values for k, weight functions, and distance metrics to find the best combination

------

RandomizedSearchCV for Faster Tuning

If GridSearch takes too long, use RandomizedSearchCV, which randomly selects combinations instead of testing all.

from sklearn.model_selection import RandomizedSearchCV

# Define hyperparameter space
param_dist = {
    'n_neighbors': range(1, 50, 2),  # Test odd values of k
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

# Run RandomizedSearchCV
random_search = RandomizedSearchCV(KNeighborsClassifier(), param_dist, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search.fit(X_train, y_train)

# Best parameters
print(f"Best Parameters: {random_search.best_params_}")
print(f"Best Accuracy: {random_search.best_score_:.4f}")

If GridSearch takes too long, use RandomizedSearchCV, which randomly selects combinations instead of testing all.

-----------------------------
Full Code: Compare Classifiers with ROC & Confusion Matrix

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc

# ðŸ“Œ Step 1: Load Data & Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# ðŸ“Œ Step 2: Define Models
models = {
    "KNN": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(max_depth=5),
    "Random Forest": RandomForestClassifier(n_estimators=100)
}

# ðŸ“Œ Step 3: Hyperparameter Tuning for KNN
param_grid = {
    'n_neighbors': range(1, 30, 2),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)
best_knn = grid_search.best_estimator_  # Best KNN model

# ðŸ“Œ Step 4: Train & Evaluate Models with Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
model_results = {}

for name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    model_results[name] = {
        "model": model,
        "accuracy": accuracy_score(y_test, y_pred),
        "conf_matrix": confusion_matrix(y_test, y_pred)
    }

# Add best KNN model to results
best_knn.fit(X_train, y_train)
y_pred_knn = best_knn.predict(X_test)
model_results["Best KNN"] = {
    "model": best_knn,
    "accuracy": accuracy_score(y_test, y_pred_knn),
    "conf_matrix": confusion_matrix(y_test, y_pred_knn)
}

# ðŸ“Œ Step 5: Display Confusion Matrices
fig, axes = plt.subplots(1, len(model_results), figsize=(15, 5))
for ax, (name, result) in zip(axes, model_results.items()):
    sns.heatmap(result["conf_matrix"], annot=True, fmt="d", cmap="Blues", ax=ax)
    ax.set_title(f"{name}\nAccuracy: {result['accuracy']:.4f}")
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")
plt.show()

# ðŸ“Œ Step 6: Plot ROC Curves for Each Model
plt.figure(figsize=(10, 6))
for name, result in model_results.items():
    model = result["model"]
    y_pred_prob = model.predict_proba(X_test)[:, 1]  # Get probability scores
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.3f})")

plt.plot([0, 1], [0, 1], 'k--', label="Random Guessing")  # Diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.legend()
plt.show()


what this does
âœ… Trains multiple classifiers (KNN, Decision Tree, Random Forest).
âœ… Tunes KNN hyperparameters using GridSearchCV.
âœ… Uses K-Fold Cross-Validation to get better performance estimates.
âœ… Generates Confusion Matrices to visualize model performance.
âœ… Plots ROC Curves for all models on the same graph.


K-Fold is NOT limited to KNN, SVM, or Neural Networks! You can use it for any model to ensure a robust evaluation
